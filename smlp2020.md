# Day One

## Basics

Setup: Julia version 1.5.1 & VS Code

VS Code "1-2-3-4"
1: search
2: folder
3: julia workspace
4: extensions

Setting up VS Code:
- make sure that the console (Cmd-J) has the right working directory with pwd
- make sure that VS code is looking at the correct version of Julia (Preferences - Settings - search for Julia:) 
  - go to Julia: Executable Path (if it blank, should be the default path)
  
## VS Code syntax
- Open terminal: Cmd-J
- Open Julia REPL: Cmd-Shift-P

## Julia package manager in VS Code
- open the package manager in VS Code with ] in the Julia REPL
- this will change the line start with pkg>
- status will show the packages available in the local enviornment, which may be specific to the folder 
- call instantiate to start the Dr Watson repo, update to update the repo packages
- close package manager with delete/backspace
- when reopening a project made with DrWatson, you may have to go into package mode and call activate, update, and status

## Julia syntax
- switch to a shell with ; -- when you run a command from shell, it'll immediately go back to julia after running

- read csv: CSV.read(datadir("filename.csv"))
- read RDS file: rcopy(R"readRDS($datadir('filename.rds'))")
- read MixedModels data: dat_MM = MixedModels.dataset(:filename)
- datadir(): open the subfolder called data in the given folder (i.e. to retreive a file)
- scriptsdir(): open the subfolder called sripts

- convert the jmd to html: using Weave, weave(scriptsdir("current_file.jmd"), doctype="md2html")
- convert the jmd to a ipynb (jupyter notebook): 
  - using Weave, IJulia
  - convert_doc(scriptsdir("current_file.jmd"), projectdir("scripts","current_file.ipynb"))
  - IJulia.notebook(dir=projectdir("scripts"))  # or jupyterlab(dir=projectdir("scripts"))
  - if you can't open it from VS Code, open a browser and go to 8899 local host
  
- ending a command with ; means it won't print the output
- commands that start with . mean that they are applied elementwise
- commands that end with ! mean that they edit in place

- arrays use square brackets
 - [1, 2, 3] == [1, 2, 3]
    - check that the whole thing is the same
  - [1, 2, 3] .== 3
    - checks elementwise, is each one a 3
    - returns 0, 0, 1

- create a dataframe for use in MixedModels: slp = MixedModels.dataset(:sleepstudy)
- return summary of dataframe (mean, min, max, etc.): describe(slp)
- return summary of column: describe(slp.reaction)

- extract columns: slp[!, :reaction]
  - ! means all rows
  - :colname pulls the column names 
  - for multiple columns: slp[!, [:reaction, :subj]]

- create new column: slp[!, :extra] .= 3 # note the . -- this is broadcasting
- extract subset: slp[(slp.reaction .< 250) .& (slp.subj .== "S308"), :]

- join columns together (as in R cbind): hcat()
- inner join two dfs: innerjoin(ranefvals, withinsubj, on = :subj);

- scale variable in column: zscore(slp.reaction) / zscore!(rt, mean(rt), std(rt))

- group by & summarize: combine(groupby(slp, :subj), :reaction => mean)
- multiple groups: combine(groupby(slp, [:subj, :days]), :reaction => mean)

## Using R in Julia
- type $ in the Julia REPL to switch to R

- using RCall -> package to be able to call R items in Julia
- @rput to send an item to R
- @rget to grab an item from R 

- use R inline with R"summary(mtc)" -> will do the same thing as it would do in R 
- use R """ for multiline R codes (triple strings before and after chunk)

## Mixed effects models
- based on presentation by Doug Bates with sleep deprivation data
- random effects are based on grouping variables, which must occur more than once (this is why it's called repeated measures)

- modeling in Julia, workflow:
  1. load in data: sleepstudy = MixedModels.dataset(:sleepstudy);
  2. look at summary stats: describe(sleepstudy)
    - show max/min, mean/meidan for numeric, and number of unique levels for categorical
  3. send data to R and set up the packages you need, if you want to do some graphing in R
    - packages: 
      R"""
      require("ggplot2", quietly=TRUE)
      require("lattice", quietly=TRUE)
      require("lme4", quietly=TRUE)
      """;
    - plot settings:
      RCall.ijulia_setdevice(MIME("image/svg+xml"), width=6, height=3.5)
    - send current df to R:
      @rput sleepstudy;
  4. use the Julia equivalent of group_by & summarize with a do-block
    - do-block to calculate standard devition by the grouping in factor_name:
      sumry = combine(groupby(df, factor_name)) do sdf
        end
    - will result in a tuple of name, value pairs
  5. fit mixed effects model in Julia
    - first define formula, looks a bit like lme4: 
    f1 =  @formula(reaction ~ 1 + days + (1+days|subj));
    - then fit
    m1 = fit(MixedModel, f1, sleepstudy)
  6. first look at random effects (slope/intercept by subj here)
    - ranefvals = DataFrame(only(raneftables(m1)))
  7. compare models
    - nested models for goodness of fit with a chi-square likelihood ratio test (like anova() in R)
      MixedModels.likelihoodratiotest(m2, m1)
    - non-nested models with AIC/BIC, here as a df
      DataFrame(map(m -> (objective = objective(m), AIC = aic(m), AICc = aicc(m), BIC = bic(m)), [m2, m1]))
  8. look at model values
    - you can extract parts of the model by calling modelname.value
    - possible values include: (:formula, :sqrtwts, :A, :L, :optsum, :θ, :theta, :β, :beta, :λ, :lambda, :stderror, :σ, :sigma, :σs, :sigmas, :b, :u, :lowerbd, :X, :y, :corr, :vcov, :PCA, :rePCA, :reterms, :feterms, :allterms, :objective, :pvalues)
  9. send the model to R as a lmer item with JellyMe4
    - use $-interpolation:
    R"summary(m1 <- $(m1, sleepstudy))"
    - then can use some R functions for plotting/looking at lmer objects
    R"""
    dotplot(ranef(m1),
    scales = list(x = list(relation = 'free')))[['subj']]
    """

## Mixed effects models in general
- always look at random effects until you have specified your model successfully
- look at fixed effects only once you have settled on a random effects structure to not bias your pick based on signfiicance
- shrinkage: when you run a mixed effects model, it assumes that you are measuring from one population, thus individual slopes/intercepts are pulled towards the group mean
- this shrinkage takes place where possible, i.e. for those subjects where the fit is adjustable, where it won't create huge residuals

- to fit a model without correlated slopes and intercepts, use zerocorr() around the random effect
  - f2 = @formula(reaction ~ 1 + days + zerocorr(1+days|subj));

## Files used today

- scripts/julia_intro.jl
- Longitudinal.jmd 



# Day Two

## Workflow in VS Code vs. R Studio
- open the jmd in VS Code
- start the REPL with Cmd J
- double check the working directory with pwd()

## DataFramesMacro package
- designed to add some tidyverse functionality to Julia
- subj_order = sort(combine(groupby(sleepstudy, :subj), :reaction => mean), :reaction_mean);
- sleepstudy = @linq sleepstudy |> transform(subj = levels!(categorical(:subj), subj_order.subj));
- "pipe": |> 

## Wrangling in Julia
cellmeans = by(dat, [:F, :P, :Q, :lQ, :lT], 
            meanRT = :rt => mean, sdRT = :rt => std, n = :rt => length,
            semean = :rt => x -> std(x)/sqrt(length(x)))

## Relevel factors in Julia
dat = @transform(dat,
                 F = levels!(:F, ["HF", "LF"]),
                 P = levels!(:P, ["rel", "unr"]),
                 Q = levels!(:Q, ["clr", "deg"]),
                lQ = levels!(:lQ, ["clr", "deg"]),
                lT = levels!(:lT, ["WD", "NW"]));

## Sum coding in Julia, i.e. EffectsCoding
cntrsts = merge(
    Dict(index => EffectsCoding() for index in (:F, :P, :Q, :lQ, :lT)),
    Dict(index => Grouping() for index in (:Subj, :Item)),
);

- First line is the coding of factors
- Grouping() denotes those factors as random - they don't need contrast coding

## Fit a model with MixedModels
m = fit(
    MixedModel,
    @formula(reaction ~ 1+days+(1+days|subj)), 
    sleepstudy,
)
- first call fit, then say that its a MixedModel, then give the formula after @formula, then give the data

Here with factor coding and a separately defined formula:
cntrsts = merge(
    Dict(index => EffectsCoding() for index in (:F, :P, :Q, :lQ, :lT)),
    Dict(index => Grouping() for index in (:Subj, :Item)),
);

m1form = @formula (-1000/rt) ~ 1+F*P*Q*lQ*lT +
                              (1+F+P+Q+lQ+lT | Subj) +
                              (1+P+Q+lQ+lT | Item);
cmplxLMM = fit(MixedModel, m1form, dat, contrasts=cntrsts);

## Extract random effects
- cms = only(ranef(m)) .+  m.β;
- the only will check that there is just one
- fixed effects with modelname.\beta 

## Create new column with repeating label
- cms.estimate = "Conditional mean"; #every row will have the label "Conditional mean"

## Conditional mean vs. within-subj vs. pooled
- Pooled: if you ignore the grouping variable (ignore subject) and compute one regression line per participant
- Within-subject: if you compute one regression line per person, but this is usually overfit
- Conditional mean: if you compute a mixed effects model -- the model looks at the population mean, and looks at the variability in subject data
  - Shrinkage / "Borrowing strength": the individual slopes and intercepts are pulled in towards the population mean
  - Thus, shrinkage controls for the unreliability in the data
  - Conditional means should be more accurate over time, i.e. if you measured this participant again under the same conditions, it will be closer to the conditional mean
  - Shrinkage doesn't just look at the global means, but much more at the stability of the data. If there isn't much variability or there are a lot of observations, they won't be pulled in much.
- These graphs of shrinkage, each participants conditional vs. within-subj means, etc. also show something about individual differences

## Basic Julia
- ! means in place
- column names start with : because they are "symbols"
- when looking at columns of a dataframe, : returns a copy of the column, ! returns the actual columns
  - i.e. df[:, :cond] vs. df[!, :cond]

- scriptsdir and projectdir("notebooks", "file") etc. are part of DrWatson

## IJulia notebooks
- once its launched with an IJulia code, you can open it in a broswer under localhost:8890

## Theory of mixed models
- Parts of a mixed models:
  - Response: DV
  - Factors: categorical / ordinal variables, discrete levels
  - Covariates: numeric predictors / continuous variables -> measured without error
- Stratification: trying to change a normal distribution into a uniform distribution, for example by taking a continuous factor and separating it into separate groups
- Statistical power is better for uniform distributions, this is why stratification is sometimes used 
- Skew: trying to change a skewed distribution into a normal distribution, i.e. by using a Box-Cox transformation 
- Covariates can have any distribution, there isn't a normality assumption
  - For example, if you measured response times from two groups (kids and college students), this would likely be a bimodal distribution
  - This is fine! When you run the model, you probably have a factor for age or group, which will take care of this and produce normally distributed residuals
- Residuals must be normally distributed, the residuals come from the DV, the DV itself and the covariates themselves don't have to be normally distributed

- Three dimensions that apply to both factors and covariates (categorical and continuous predictors):
  - between-units vs. within-units
    - i.e. between-subjects, between-items, between-groups
    - does not matter for means and standard deviations 
    - does matter when you compute standard errors and confidence intervals 
    - between-unit factor: a unit occupies exactly one level of the factor, i.e. every subject occupies one level of the factor gender
    - within-unit factor: a unit ideally occupies every (or at least more than one) level of the factor, i.e. every subject is exposed to every level of the condition manipulation
    - between-unit covariate: a unit occupies one value, i.e. every subject has a unique age and only one age
    - within-unit covariate: a unit occupies is exposed to all or multiple values, i.e. subjects all see stimuli at different presentation durations
    - ex: gender 
      - between-subject factor
      - within-item factor -> each item is seen by different genders
    - ex: word frequency
      - within-subject covariate -> each participant sees different frequencies
      - between-items covariate -> each item has only one frequency
    - only within-unit factors are allowed in the mixed effects, but the model will allow it if you mess it up so be careful
  - fixed vs. random 
    - random factors: when we sample levels of the population randomly, i.e. random subjects from a population of college students
      - issue of generalizability - how confident are you that the sample you took is random / representative of the population
      - also applies to items - the sentences you picked are a random sample of sentences from the population of sentences
      - we need to be clear about what the population is
      - trying to generalize from the levels of the random factor to the population from which the sample was drawn
      - in a mixed model, there should be a sufficiently large number of levels
    - fixed factors: you can't claim that you can generalize to levels that are not in the factor
      - we have a discrete set of levels and we don't generalize beyond those levels
      - if you have a small amount of subjects, ex: 5, then you don't really want to generalize from these to the whole population, so it might be better to treat them as fixed factors
      - whats important here is variance, if you have too few, then you can't realiably calculate variance, so better to treat it as a fixed factor
      - for practical purposes, it might not really make sense to have a mixed effect for text if there are only 5 texts
      - this is up to your judgement -- do you have enough information to estimate the variance associated with the population?
      - so there are cases in which the factor may be a random factor, i.e. a random sample of the population, but if there are too few levels, then in a mixed model, it might not make sense to use it
      as a random factor because there isn't enough info to reliabily calculate the variance
    - fixed covariates:
      - we do generalize between the minimum and maximum, since there are not discrete levels 
      - this is done by a linear model, which will guess at the chance based on a change one level
      - thus, it's not the case that you can only measure the actual levels/numbers you have, but we do generalize between the min and max, by every tiny step in between
    - difference should be made between fixed effects that you are interested in and fixed covariates, that you wish to control for but are not interested in for theoretical reasons
    - example from / common pitfall in psycholinguistic experiments:
      - if you have the same sentence with two different words in it, i.e. a high frequency and a low frequency one
      - if you take into account whether the word is high or low frequency, for example through a fixed effect for frequency, then you are left with only one item
      - conceptually the same as if you have one subject who reads high or low frequency words, you still have one subject, not two
      - not saying that you have to see it this way, but that it could be this way, and it's something you have to think about 
    - example of trial number:
      - when you add trial number as a fixed effect to the model, you have to commit to the form that you think it takes, i.e. that it is linear
      - it could also have another form: cubic, quadratic, formless, splines / smooths
      - you cannot have a random covariate (only a random factor, i.e. only categorical)
      - you could put trial number as a random factor, which would discretize it, and the variance there would be removed though without any order
      - not recommended to do this, but it is something to think about
      - point is that for covariates, you have to commit to a fixed form if you add this as a fixed effect. if you just add it as a fixed effect, you are stuck with the assumption that it's just a linear effect
  - experimental vs. quasi-experimental
    - experimental factors/covariates: when the experimenter assigns the units of a random factor to the levels of the factors/covariates
      - you decide what presentation time is given to which subject/which object, i.e. the presentation time is under your experimental control 
    - quasi-experimental factors/covariates: the features that subjects or objects bring to your experiment that are not under your control
      - quasi-experimental items feature: word frequency, word length -> you can't assign a word length to a word just for your experiment
      - quasi-experimental subject features: gender, age

## Factor coding
- think about the direction of your effect, and set the faster level as the base level -> then if it is positive, you know that the effect is in the correct direction
- in this way, you can also assume the direction of the correlation effect - so if the correlation is positive, then as one increases, the other increases
  - ex: participants who show a big frequency effect also show a big priming effect
- a mixed model is not an exploratory device! so the data should come back in a way that either confirms or disconfirms your expectations
  - factor coding in a way that makes sense assists in this
- always specify your contrast coding -- make sure it is a line in the actual code even if it is the default

## Model selection
- don't look at fixed effects during model selection -- just look at random effect structure
  - fixed effects rarely make a different to random effects
  - guards against using significance to make decisions in model specification 
- for example, look at the variance-covariance matrix:
  - in Julia: VarCorr(cmplxLMM)
  - are there small std. deviations?
  - are there correlations that are close to 1 or -1?
    - correlations of close to 1 (i.e. also .8), this could be at the boundary 
- you can also look at PCA:
  - in Julia: cmplxLMM.PCA -> where cmplxLMM is the model name
- it's not enough that the model converges, but it should also be a model that is supported by the data
- "overparameterized": the random effects structure does not have any parameters at the boundary, it can identify all parameters
- outliers: should maybe be removed after the model is run. run the model, run diagnostics, and then see if there is a huge residual - then it is reasonable to remove it
- in a complex design, the first step is to take out the correlation parameters, because often you need a huge number of items / participants to estimate them
  - you need reliable evidence to determine that they are 0, so if there isn't enough, they might return a non-0, but this doesn't necessarily mean anything
  - to run a model without correlations between mixed effect slopes and intercepts, surround the whole random effects term with zerocorr()
    m2form = @formula (-1000/rt) ~ 1 + F*P*Q*lQ*lT + zerocorr(1+F+P+Q+lQ+lT | Subj) + zerocorr(1+P+Q+lQ+lT | Item);
  
## Slope - intercept correlations
- it takes more data in general to estimate slope-intercept correlations in the random effects
- thus a good step in reducing the complexity of a model is not to estimate these (i.e. with zerocorr())

## Nested vs. crossed random effects
- nested random effects: every participant is a student who is in one class in one school
  - strictly, each student stays in one class and every class stays in one school
- crossed random effects: every participant sees every item in every condition
  - strictly, this would mean that there is no missing data (no eye blinks in eye tracking for example)
- in most cases, we have partially nested or paritally crossed random effects

## Mixed models: Convergence & Singularity
- rpubs.com/palday -> convergence warnings in lme4

### Fitting models with maximum liklihood, REML, or deviance
- maximum likelihood estimatation (MLE): the model tries to find the values of the model parameters that maximize the likelihood of the data given the model 
- thus it tries to find a model that would produce the data that you found

- REML: residualized maximum likelihood is not THE likelihood, advised against using 

- deviance: -2 * log(likelihood)
- maximizing the liklihood = minimizing the deviance
- comes up in the Chi-square test, i.e. via anova() in comparing models 
- standard deviation is the square root of the variance

- only the random effects affect the likelihood / REML / deviance
- thus, when you fit the model, you should really worry about mixed effects

- when fitting an intercept only model:
  - what is stored is the standard deviation by subject, and the standard deviation in residuals
  - the models tries to minimize the ratio of deviation by random effects and deviation in residuals 

- when fitting a slope-intercept model:
  - now there are 3 "knobs to turn": intercept, slope and correlation parameter between the two 
  - thus complexity quickly starts to build up 
  - models with more parameters have less deviance in general

- fitting a model:
  - in mixed effects models, the optimizer tries to maximize the likelihood or minimize the deviance
  - it does this not by estimating the random effect for participant, but in trying different values for the RATIO between standard deviation for
  random effects for participants (or whatever) and standard deviation for the deviance (the variation not described by the model)
  - you could also get a standard deviation of 0 for a random effect, which would mean that there is no variance by subject that is not just residuals / not just noise

- zero correlation models:
  - when you don't estimate the correlation term, you actually are telling the model to assume that it is 0
  - correlation parameters can tell you something about your data, kind of like an interaction term
    - for example, they might show that slow responders respond more strongly to your manipulation, i.e. to frequency
    - thus, they can show strategies, individual differences, or things about your design
  - a very tiny random effect with a large correlation to other random effects -> might still be reliable because it correlates with other parameters or because its spread from positive to negative

- optimization failures (more of a problem in lme4 than in MixedModels.jl)
  - optimizers have to have some notion of whether or not they've converged or else it wouldn't know when to stop
  - checking derivatives:
    - JellyMe4 sets check derivatives to false 
    - checking derivatives means checking the rate of change: when you are at the trough (bottom), you're not moving at all, not going up or down
    - this is slow and due to rounding, inaccurate -> in binary, 0.1 is an infinite number, so it has to be truncated a bit (like .33333 in decimal)
      - thus, most computing languages, 0.3 + 0.3 + 0.3 = 0.99999999999999999999999
      - thus absolute equality is difficult or impossible based on this computing structure
    - this is an issue with using derivatives for optimizing

- lme4 warning about singular fit or boundary fit, gradient not equal to 0:
  - if you get to a boundary, i.e. the edge of the space where the model is making predictions, you'll get a local minimum/ a minimum but not a real bottom
  - boundary fits occur when one or more of the parameters / random effects goes to 0 -> i.e. between subject variance is 0
  - variances are squared quantities and can't be smaller than 0
  - thus, at 0, you're at the edge of the possible values
  - then, mathematically, the matrix is singular
  - a singular fit is mathetmatically well-defined, not a problem per say
  - two big problems with singular fits:
    1. just because a random effect goes to 0 does not mean it's actually 0, just means that you can't distinguish the variance compared to the noise
      - this could also be due to low statistical power 
    2. slow to compute 
  - singular fits could be indicative of overfitting 
  - singular fits are more likely with maximal models -> they have lots of random effects / complicated strucutre, so its likely that some things will go to 0
  - the singularity itself is not a problem, the issue is that it can be indicative of other issues:
    - maximal models -> not enough data to estimate those things
    - try to estimate something that is between-items

- when you start of with the model, think about where you expect variability: your model is a statement on your expectations
- phillip alday: include lots of nuisance variability -> think about if subjects might differ in these things; prune downwards
- doug bates: if a fixed effect is large, it's likely that participants or items might differ in their reaction to it

- doug bates: dale barr et als point -> if you have a variable in the model just to control for nuisance variables, then you might as well put them in as mixed effects to guard against "missing something"
  - are you only interested in fixed effects? or is the subject-to-subject variability of interest to you?
  - if you just are saying that there is something you can't control for in the experiment but want to control for in the analysis, then singularity is not a really big concern
  - if you are trying to control for repetition/blocking, singularity isn't a huge issue -- however you if you are trying to interpret then it becomes more problematic

## Bootstrapping
- start by generating random numbers -- requires "Random" package
  rng = MersenneTwister(1234321);
- feed the random numbers, the number of bootstraps (here 10,000) and the model
  samp = parametricbootstrap(rng, 10_000, m1);
- show as dataframe
  df = DataFrame(samp.allpars);
- gives estimates for β (fixed effects), σ (standard deviation) for the random effect term (by name, in this example, batch) and σ (standard deviation) for the residuals
  - there may also be a p, the correlation coefficient between the slopes and intercepts
- can pull out the standard deviation for the residual with this filter-like @where macro -> and pull out the value
  σres = @where(df, :type .== "σ", :group .== "residual").value
  - or you can pull it out as it is a defined component of the item:
  samp.σ where samp is your bootstrap item
- and plot it:
  plot(x = σres, Geom.density, Guide.xlabel("Parametric bootstrap estimates of σ"))

- the example of dyestuff shows a peak at 0, i.e. several bootstrapped samples could not distinguish batch / random-effect-based variation from random noise
- thus, you might conclude that including batch as a mixed effect is reasonable but can't always be estimate well

- shortest coverage interval: shows the bootstrapped estimates in the most dense/short way
- in this example, the coverage interval includes 0 as the minimum -> this means that we need more batches
- the coverage interval can also show you what things are bein well or pooly estimated
  - in the example, a coverage interval for one of the correlation coefficients was from -.4 to 1.0 -> correlation can only vary between -1 and 1
  so we know this is a bad estimation and can safely be dropped
  - bottom line from these examples is that you need a lot of datapoints to accurately estimate the correlation coefficients
  - but also that you can use bootstrapping to figure out how well its being estimated, by seeing how much variance is in the bootstrapped estimates
- you can also extract how many of the bootstrapped samples were singular: 
  sum(issingular(samp2)) -> where samp2 is the bootstrapping object

- overall takeaway: parameters and standard deviation are relatively well-behaved even in small datasets, but estimating correlations require much more data

## Gadfly
- a bit like ggplot, a plotting package for Julia

## Getting a dataset from R and doing some subsetting on it
orthofem = rcopy(R"""subset(nlme::Orthodont, Sex == "Female", -Sex)""");

## Correlation coefficients
- if you do linear transformation to the predictors, this changes the correlation parameters, i.e. scaling or centering
- also is affected if you transform your DV  

## Questions/answers 
From Phillip Alday to Me: (Privately) (15:47)
Basically the idea is that you extract a subset of the contrasts for a given factor from your fixed effects and use them in your random effects. However, there is no easy way to express this in the standard contrasts route, so Reinhold creates the contrasts by hand and copies them over.
Re overparameterization: Basically it means that the random effects are more complicated than you can estimate from the data and so the shrinkage kicks in and one or more random effect goes to zero, giving you a singular/boundary fit.

When would I (rightfully) assume that the correlation between my random effects is 0 and therefore choose to run a zerocorr model?
From Phillip Alday to Everyone: (15:49)
@Marleen: good question. I usualy don't use this technique, but it's often used as a first step when trying to reduce random-effects complexity. RE complexity is usually judged based on the presence of zeros in the RE estimates OR the rePCA indicating that a small number of components explains most of the RE (more on this trick tomorrow).

## Files used today
Showing the workflow of switching from RStudio to VSCode
- sleepstudy_borrowingstrength_lme4.Rmd
- sleepstudy_borrowingstrength_lme4.jmd
- sleepstudy_borrowingstrength_MM.jmd 

Conceptual info on mixed models
- MRK17_spcfctn_slctn
- notes in papers/0_2_SMLP2020_Stream4_factors_covariates

Bootstrapping
- Bootstrap.jmd / Bootstrap.ipynb

# Day Three

## Using an R function in Julia that you wrote yourself
source($srcdir('LMM_residuals.R'))
- or with full file path (I imagine, $srcdir is a DrWatson function)

## Centering numeric covariates
- centering variables is even more important in mixed models because of the correlation terms 
- the problem with centering variables is that its highly specific to your dataset and may not generalize at all to other samples
- it might also be worth considering centering variables on something that is not specific to your dataset
  - ex: centering word length on the average word length in the language, i.e. 6 in German
- however, if you have two groups like "normal" and dsylexic children, you might not want to center over the two groups 

## Interactions
- interactions are always test of parallelility 

## Quadratic trends
- ex: if you expect gaze fixations to be longer in the middle of the word as on either side

## Treatment contrast coding
- makes sense to use when you have one group that it a control group and another that is a treatment or treatment groups 
- then interpretation becomes (in the idea that normal is control and dsylexic is treatment):
  - intercept: mean speed for normal children
  - freq main effect: effect for normal children
  - group main effect: how different are the dsylexic children from the normal children
  - group by freq interaction: how the slope of the dsylexic children differs from the slope of the normal children

### Set contrast group in Julia
dat = @transform(dat, Group = levels!(:Group, ["norm", "dyslexia"]))

- then: 
  R"contrasts(dat$Group) <- contr.treatment(2, base=2)"

## Quadratic trends
- quadratic trends also become smaller, thus it might not totally make sense to hypothesize a strictly linear trend
- i.e. reading times get smaller as frequency goes up -- this could also be true in a quadratic sense

## Workflow Kliegl
1. walk through every covariate/factor/predictor and think about which groups it is between- and which within-units. Also, think about what kind of trends they would be (strictly linear, or maybe quadratic?)
2. put everything in that makes sense -> all between-subject things as a by-subject random effects 
3. start with a zerocorr model
4. don't look at the fit but at the variance correlation parameters with VarCorr(modelname)
  - by assuming that the correlation parameters of zero, this will allow some variance components that are not well supported by the data towards 0
    - this doesn't mean there is no variance but it means that you cannot distinguish the variance from the noise in the model
    - this makes it unlikely that this variance will correlate with anything else (it will be a constant)
    - so its a good choice to throw it out
  - this is showing only the random effects
5. look at the principal components with modelname.PCA to check for overparameterization (or modelname.rePCA)
  - look at the normalized cumulative variances for each of the groups you've set random effects for, where this hits 1.0, you can see where the model becomes overmarameterized
  - i.e. you need to remove some of the mixed effects if the cumulative variance hits 1.0
  - basically, this is checking that if you combine variables, how many would you need to account for all the variance described by all variables 
  - if its overparameterized, remove some of the variables where the VarCorr show 0 or close to 0
  - correlation parameters on subject vs those on item, etc., are unrelated -- it's not all or nothing
    - for example, you might want to keep them in for subject, not for items and words if these are at the control level -- because you are interested in differences between subjects
6. check model fit between the full model and the one where you have removed the unneccessary factors with MixedModels.likelihoodratiotest(model1, model2)
  - if there is a non-significant chi-square then you know that the reduced model doesn't fit any worse
  - just be aware that mathematicians might recommend against doing a likelihood ratio test on overparameterized models, because the test assumes that all models that are fed to it are equally well-formed
7. add the correlation parameters back in (remove the zerocorr) and take a look with VarCorr
8. use PCA to see if the model is now overparametrized -> look at the normalized cumulative variances
9. if it is overparametrized, go back and look for small std. dev. and use your domain knowledge for what to remove

## Correlation terms
- show individual differences in the difference in experimental effects 
- throwing out the correlation terms ignores the expeirmental effects 

## Intercept being 0 variance
- if you think of the slope and intercept as a ramp, you can visualize how slope and starting point interact 
- so if you get 0 variance for the intercept 

## How to get a mixed model wrong (Alday)
- if you leave out random effects that you will see st. errors that are too small -> you will be attributing more of the variance to the effects than really come from the effects (might be coming from these grouping variables) -> your p-values will be too small 
- if you put too many random effects, your model will be overparametrized and basically "memorize" your data -> your results won't generalize
- the "new statistics" that says don't focus only on significance, is now 10 years old
- significance testing is where misspecification starts to become a problem
- start thinking about what your model says about your data and about the world
  - look at the fitted vs. the observed -> does your model summarize the patterns in your data in a meaningful way
  - people can replicate predictions (quantitative predictions) 
- as long as you have your grouping variables right, then everything else comes down to type 1 error rate -> and type 1 error rate isn't what you should be focused on
- the keep it maximal strategy will make sure that you don't make type 1 errors at your significance level -> this is important if what you're really worried about is your (ultimately) arbitrary alpha level
  - but it will affect your type 2 error rate and the generalizability of the model to new data / the world
- phillips takeaway: be more concerned with how your model says something about the world than in signficance values -> rather not overfit even though overfitting might protect against some false positives

## Checking assumptions
- if your assumptions aren't met, your standard errors will be wrong -> then your p-values will be wrong and your t-scores
- bootstrap confidence intervals will help here, but this is also based on normality assumptions 
- you also look at your residuals after the model is run, you can also take out outliers after you've run it too
- for small sample sizes, a distribution might not look normal just because it's too small; for large sample sizes, any sort of mathematical calculation of sample sizes (shapiro wilk, etc.) will reject that it is normal for small deviations and no distribution is really perfectly normal
- however, if you have something that you KNOW isn't normally distributed (eeg across the scalp) -> then it doesn't make sense to count that thing (the electrode) as a grouping variable

## Parametric vs. non-parametric bootstrap
- parametric bootstrap -> generate data with a model (that has parameters, i.e. parametric)
- non-parametric bootstrap -> resampling from the dataset 
- parametric bootstrapping retains complicated nested structures, etc. 

## Caterpillar plots
- http://documentation.sas.com/?docsetId=statug&docsetTarget=statug_mcmc_details56.htm&docsetVersion=15.1&locale=en

## Zerocorr models
- you can also wrap subsets of random effets in zerocorr()
- i.e. zerocorr(0 + freq | subj) + (1 + trial_num | subj)

## Preprocessing in Julia
- StatsBase package has the function zscore() to center/scale predictors
- define contrasts as a dictionary:
  contr = Dict(:column_name => SeqDiffCoding(), # sequential difference coding
                :subj => Grouping() ) # denotes this as a grouping variable
- StatsModel package has a lot of different options for contrast coding (check out their Github), also check out Schad et al. 2020

## Model syntax in Julia
- write interactions: factor1 * (factor2 + factor3) will compute interaction of 1 and 2 and of 1 and 3
- Julia doesn't mixed named and positional arguments -> you can't use names to change the order of positional arguments, and named arguments have to come at the very end
- log. regressions in Julia are called with Bernoulli() if the outcome is 0/1 or binomial if the outcome is a proportion 

## Generalized linear models (logistic regression, etc.)
- has some extra options in Julia, i.e. fast= true (can use it for power simulations for example, fits mostly based on random effects)
- nAGQ (look it up)

## PCA
- if you set correlations to 0, this removes a LOT of parameters, because the correlation matrix grows quadratically
- if you run modelname.rePCA, you get the amount of variance described by the random effects:
  - i.e. [.5, .7, .95, 1.0]
  - the first random effect accounts for 50% of the variance that all random effects describe
  - the second one accounts for 70% of the total variance, so 20% more than if you just had one random effect 
- every parameter takes up information -> the more we estimate, the more information is "consumed"
  - if we run out of information -> the estimates shrink to 0
- http://doingbayesiandataanalysis.blogspot.com/2019/07/shrinkage-in-hierarchical-models-random.html
- PCA will in theory "mix" up the components and tell you that if you had the right mixture, how many components youd need. thus its not going to be like one-to-one, but does give you some info 
- PCA picks out orthogonal components, i.e. components that are at right angles, example of a zeppelin

## Data manipulation in Julia
- in src/utilies.jl there is a function to prepend numeric array into one prepended with a tag, S by default, in a meaningful order
  - i.e. 1-100 as S001, S002; 1-10 as S01, S02, etc.
- anonymous functions to make new columns (takes Item column, creates item column using function taggedstring()):
  : Item => (x -> taggedstring(x, 'I')) => :item
- cloze scores with lots of 0s in example -> might not make sense to center the variable
- to rename labels of a column, which is currently labeled as 0/1 for yes/no:
  transform!(df, :Accuracy => (x -> ifelse.(iszero.(x), "N", "Y")))
- to rename a column use select like tidyverse (order preserved):
  select!(df, :subj, :item, :Condition => :cond, :correct, :ReactionTime => :rt)
- PooledArrays -> roughly correspond to factors in R (CategoricalArrays are similar, but are much less inefficient, so might go away)
- Julia and R have differences in the way they represent numeric values in bytes -> there are more options in Julia, some of these won't be able to be read into R correctly
- groupby & combine are like group_by & summarize in tidyverse
- another way to rename levels would be recode() or recode! -> look it up if needed

## Files used today
- example on eye movements in dsylexia, Kliegl's example of his process of model selection:
  Lopukhina_dyslx_rk.jmd
- Phillip's example with log. regression data:
  Tal-Perry_intervals_db.jmd
- Doug Bates' example with cloze, reaction times, etc. on wrangling in Julia 
  CSVfiles.jmd

# Day Four

## Factor coding
- always specifically document the factor levels you have and which what order, even if it's already the default order
  example here with column CTR from df dat1, the order they are written in the list is the order they're factored
  dat1 = @linq dat1 |>
        transform(CTR = levels!(categorical(:CTR), ["val", "sod", "dos", "dod"])); 
- then you can also check the grand means before you get into modeling
  cellmeans = by(dat1, [:CTR], 
            meanRT = :rt => mean, sdRT = :rt => std, n = :rt => length,
            semean = :rt => x -> std(x)/sqrt(length(x)))
- here we're interested in contrasts that are right next to each other sequentially so we use sequential difference coding 
  - for this, set up a dictionary where you define a coding scheme for a symbol, i.e. you don't even really mention your dataset in this command
  - you also have the chance here to define factor levels (technically don't have to do it again if you've done it with the explicit line above)
  cntr1 = Dict(
    :CTR  => SeqDiffCoding(levels=["val", "sod", "dos", "dod"]),
    :Subj => Grouping()
  );
- you have to remember that the model looks at contrasts not at levels of a factor -> in the example above with the factor with four levels, what we're really looking at is three contrasts (val to sod, sod to dos, dos to dod)
- shrinkage will be affected by unbalanced designs -> in the example above, the baseline level (val) has way more observations than the other three categories (70-10-10-10%) -> thus we'd be dealing with one reliable mean/sample compared to three less reliable means/samples 
  - however with a decently large sample, this will probably have a pretty minor effect (diminishing returns, 100001-100002 is less meaningful than 101-102)
- the model output will show the levels, but you have to keep in mind that it's just showing a difference from the level before it to this level -> in general always keep the coding scheme in mind although it might not be very obvious from the model output (i.e. it will say CTR: dod although it is measuring the difference from dos to dod)
- the intercept here is the grand mean (so not the mean of the observations but the mean of conditions (/level) (mean of cond1 + mean of cond2 + mean of cond3 + mean of cond4 / 4)) thus it will not be affected by the number of data points in each condition -> the overall mean in the example above would be strongly drawn towards the biggest group, the 70%
  OM = mean(dat1.rt)             # mean of observations
  GM = mean(cellmeans.meanRT)    # grand mean = mean of conditions
- hypothesis coding, i.e. how you freestyle it, this doesn't work in R: 
  cntr1b = Dict(
    :CTR => HypothesisCoding([-1  1  0  0
                               0 -1  1  0
                               0  0 -1  1],
            levels=["val", "sod",  "dos", "dod"])
  );
- the rows in hypothesis coding should sum to zero so that the intercept is the grand means
- you can include the Grouping() coding, but if you forget it, it's will still work as long as the column is categorical
- dummy coding -- the default:
  cntr2 = Dict(:CTR => DummyCoding(base= "val"));
- in dummy coding, the intercept returns the mean of the first group, the reference group or val here
- the coefficients change because they show the offsets to the first group
- you have to be careful with factor coding that doesn't use 1s and -1 but 1/2 and -1/2 or whatever else, because an interaction between them will multiply the effects, and this will get confusing fast and will quickly get weird
- effects coding: contr.sum() in R
  cntr3 = Dict(:CTR => EffectsCoding(base= "val"));
- gives the difference from the grand mean for 3 or the 4 levels -> the 4th can be computed by hand, you can set this one (the one you'd have to calculate yourself) as base="lvl_name"
- helmert coding: 
  cntr4 = Dict(:CTR => HelmertCoding());
- would be:
  [-1 -1 -1
    1 -1  1
    0  2 -1
    0  0  3]
- gives the difference to the SUM of the previous levels
- it should rather be: 
  man_helm2 = [-1    1    0   0
            -1/2 -1/2   1   0
            -1/3 -1/3 -1/3  1 ]
  so that things add up to 0, this will be rescaled to give the expected effect sizes

## Interactions
- you always have to keep in mind that interactions are just testing if two lines are PARALLEL
- example of 3x3 design:
  - plot A1, A2, A3 along the xaxis and B1, B2 and B3 along the y axis and connect these dots -> these B lines will be tested for parallelism
- contrasts are not orthogonal if you use the same line in different contexts -> then they are correlated in this way -> this reduces power 
- however, if your a priori hypothesis is in line with these contrasts, then you'll have more power to prove this actual hypothesis in the long run
- effect/sum coding and helmert coding are orthogonal; sequential differnece and treatment coding aren't orthogonal
- for nonorthogonal contrasts you should adjust your alpha level for multiple comparisons

## Quadratic trends
- center a predictor so that the meaningful middle is at 0
- if you then cube this, you get a quadratic effect, because the things far below the mean (the negatives) will become strongly positive, as will the things far above the mean
- you can then fit an interaction of your condition alternation against these two trends 
  - condition * (centered_duration + quadratic_duration)
  - then you see which of these terms (condition1 + centered_duration vs. condition1 + quadratic duration and likewise with condition2)
  - this will ultimately show which of these trends (linear or quadratic) is a better fit to the data
- you can do post-hoc LMMs if you don't interpret the significance terms (says Kliegl) -- some debate about emmeans
  - Kliegl wouldn't want to take a subset for post-hoc LMMs because you need a lot of data to estimate variance components and correlation parameters -> will make your estimates less precise to throw some data away 

## Post-hoc LMMs
- Kliegl thinks that doing post-hoc LMMs is ok, as long as you don't report/interpret p-values
- for example, if a reviewer asks about a different set of contrasts or a different interaction, etc. 

## Transforming variables
- transforming variables makes sense if the underlying principle is thought out and make sense
- for example, log-transforming RTs makes sense because human perception is shown to be logarathimic, i.e. hearing a sound that is 10x louder will often be percieved as something twice as loud -> decibals are also in a log-scale
- log-transforming changes things from an additive way to a linear way
- however, transformations change the kind of statements you can make
- gamma regression has been suggested to be suitable for RTs -> their argument is that you don't have to transform
  - gamme regression doesn't work on MixedModels.jl and is not tested as much in lme4

## DrWatson
- the Mainfest.toml is a file in a markdown language that contains the packages and their exact versions 
- when you activate a project, it finds those packages and their exact versions
- the project has the basic info and the manifest has the very specific info
- full documentation on JuliaHub (with all other packages, or at least the official ones)
- when you establish a DrWatson project, it will also generate a lot of folders, like scripts, notebooks, data, etc. 
- start a project for the first time with initialize_project(path)
- you can start with quickactivate(path) or with -- project at the top of the file
- DrWatson includes the functions projectdir() -> returns name/filepath of current project
  - similarly srcdir() and sciriptsdir()
  - you can also use these in filepaths, like scriptsdir("filename.jmd")
- DrWatson allows you to quickly activate a local project, the package versions, and also access the scriptsdir, etc. 

## Julia preproccesing
- check for missing values in julia with ismissing.(x) <- note the dot for element-wise
- .!ismissing.(x) <- notes the two dots, dot before operators or after functions 
  - this will find non-missing values
- to subset out the NAs, we can use the non-missings as an index to the df
  - x[.!ismissing.(x)]
  - but this won't change the array type
- command: disallowmissing(x[.!ismissing.(x)])
  - will change the array type if there are no missing values left
- then you can add those values back in for the non-missing values
- x[.!ismissing.(x)] = zscore(disallowmissing(x[.!ismissing.(x)]))

- or just start by dropping the rows for which any column is missing dropmissing(df)
- or just for some columns (df, [:column1, :column2])
- then you also have to get rid of the missing support
- use nrow before and after to see how many you're actually losing

## Zerocorr models
- shouldn't look at PCA on zerocorr models -> should look at VarCorr 
- from zerocorr, it'll be like .25, .5, .75, 1 in a model with 4 components
- if there are zero is in the VarCorr then this is an easy candidate to drop 

## Correlation term of 1.0
- indicates boundary fits -> might not be good to keep this term in
- probably overparameterized and probably the source of the overparameterization 

## EEG analysis
- LMMs can be used to deal with outliers, like for example blinks or alpha being too high
- preprocess normally, but you don't have to subtract out the baseline interval (alday has a paper on this 2019 psychophysiology), instead include it as a predictor in the model 
- you can still do some artefact rejection before processing anyway 

## Julia
- CSV.File provides a structure for reading a csv file, you want to convert this to a dataframe too
  - preferred to separately convert this so DataFrame(CSV.File(".."))
- readdir returns an iterator so you can also use 
  for f in readdir(dir_path)
  CSV.File(joinpath(dir, f), ...)
  end
- can also joinpath(dir, name) to join files correctly 
- SQL and other database languages use relational databases that are oriented to rows that fulfils schemas -> row tables
- R and dataframe based langauges use column-based databases that are the same length and set to a certain time 
- so the DataFrame is the column-based version
- get the filename without the extension:
  - splitext("filename")
  - first(splitext("filename.csv")) -> returns "filename"
  - last(splitext("filename.csv")) -> returns ".csv"
- filter(x -> last(splitext(x)) in (".csv", ".tsv"), readdir(datadir())) 
  - tuples work fine for just a couple arguments 
  - if you have a lot, use a vector with square brackets


## Files used today
- Kliegl's example of contrast coding using visual cue data
  kwdyz_contrasts.jmd
- Kliegl's example of quadratic trends with interval logistic regression data
  Tal-Perry_intervals_rk.jmd

# Day Five

## New coding scheme
The contrasts returned by `DummyCoding` may be exactly what you want. Can't we have them, but have the intercept return the GM rather than the mean of the base level?  Yes, we can!  I call this "You can have your cake and eat it, too"-Coding (YchycaeitCoding). 
  cntr2b = Dict(
      :CTR => HypothesisCoding([-1  1  0  0
                                -1  0  1  0
                                -1  0  0  1],
              levels=["val", "sod",  "dos", "dod"])
  );

## Plotting effects in R
- effects package in R
- remef package in R

## JellyMe4
- Gadfly plotting in Julia: plot(x=fitted(m1), y=residuals(m1), Geom.density2d) #not as good as ggplot yet
- you can call random effects numbers by m2.theta, and then you can save these and start a big model fitting from these points -> if you've copied them wrong, it'll be a bit slower, but if you've done it right, it's way faster
- when moving to R, you have to move both the dataframe and the model object 
- m1r = (m1, mb1a) #tuple that wraps the fitted model and the dataframe
  @rput m1r; #send this wrapped tuple to R
- you can check by calling R"summary(m1r)" 
- you don't have to unpack this tuple, it's kind of normal in R to have the dataframe and the model wrapped together
- what you get in R will actually be an lmer object, so you can use everything that you would normally do with lmer output (car package, etc. etc.)

## LMMs in general
- look at means, st. deviations, and number of observations by condition in Julia
  cellmeans = by(kb07, [:spkr, :prec, :load], 
    meanRT = :rt_trunc => mean, sdRT = :rt_trunc => std, n = :rt_trunc => length,
    semean = :rt_trunc => x -> std(x)/sqrt(length(x))
  )
- theres not a p-value for "speaker" or any other predictor -> there are only statistics in relationship to differences, difference in condition, etc. 
- this is why setting and understanding your contrasts is so important
- helmert coding, and others like sum coding (I think) sets the intercept as the grand mean (I think), i.e. the intercept is the estimate if we don't know what any of the levels are
  - thus, each coefficient is a step from this not-knowing to the correct level; the steps between the two levels is 2x the estimate

## Complexity
- it takes longer to fit models with more parameters, especially in the random effects

## VS Code
- you can also open the VS Code options with Cmd Shift P and choose Markdown: preview to preview Markdown files

## Power simulations
- you shouldn't simulate data to see if you had enough power, i.e. to calculate power, it won't be right 
- first define subj_between and items_between as Dictionaries 
- simdat_crossed(n_subj, n_items, subj_btwn = subj_between,  items_btwn = items_between)
- then run a parametricbootstrap(MersenneTwister(random_number), repetitions, model, /beta = [intercept, effect sizes you expect for the coefficients in order , including interaction terms], /stdeviationicon = std.dev, /theta = [ std.deviations divided by residual, like the correlation terms, this isn't really easy to simulate unless you have a zerocorr model])
- then DataFrame(bootstrapitemname.coefpvalues) <- look at the p-values associated with the bootstrap iterations 
- might be easier to set up the simulation matrix in R and then run the simulations in julia

## Files used today
- Phillip showing JellyMe4
  using_jellyme4.ipynb
- Doug Bates about simulating LMMs
  Complexity.jmd AKA Bates.Nextjournal.2020.md
  https://nextjournal.com/dmbates/complexity-in-fitting-linear-mixed-models
- Lisa DeBruine's tutorial on power simulation for LMMs
  https://github.com/RePsychLing/sim-tutorial
